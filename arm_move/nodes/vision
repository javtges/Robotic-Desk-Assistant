#!/usr/bin/env python
'''
Creates 3 services to move the arm to various positions (Waypoints).

No publishers or subscribers in this node.

SERVICES:
  + /px100/reset ~ Moves the arm to the home position, and spawns an Intel Realsense box into the Rviz scene in a defined location. Has an option to clear all the waypoints in the parameter server.
  + /px100/step ~ Moves the gripper to the desired xyz position, with a desired gripper state (open/closed). If movement is possible, it is executed and added to the waypoints list. Otherwise an errorcode is returned.
  + /px100/follow ~ Moves the gripper sequentially to the waypoints in the parameter server. If the repeat setting is true, it continues this loop until commanded to stop.

PARAMETERS:
  + /px100/waypoints ~ A list of waypoints [x,y,z,gripper_state]. This in the launchfile is initially loaded from a YAML file, though can be cleared with the /px100/reset service.

'''

import sys
import copy

import rospy
import moveit_commander
import moveit_msgs.msg
import geometry_msgs.msg
from std_msgs.msg import String
from std_srvs.srv import Empty, EmptyResponse
from moveit_commander.conversions import pose_to_list
from interbotix_xs_modules.arm import InterbotixManipulatorXS
from arm_move.srv import step, follow, reset, resetResponse

from cv_bridge import CvBridge, CvBridgeError
from sensor_msgs.msg import Image, CompressedImage
import numpy as np
import cv2
import pyrealsense2 as rs
import mediapipe as mp


class Vision:

    def __init__(self):

        moveit_commander.roscpp_initialize(sys.argv)
        rospy.init_node("vision", anonymous=True, log_level=rospy.DEBUG)

        rospy.logerr("started the node")

        rospy.Subscriber("/camera/aligned_depth_to_color/image_raw", Image, callback=self.convert_depth_image, queue_size=1)
        rospy.Subscriber("/camera/color/image_raw", Image, callback=self.convert_color_image, queue_size=1)
        self.reset_service = rospy.ServiceProxy('/px100/reset', reset)
        self.step_service = rospy.ServiceProxy('/px100/step', step)

        rospy.logerr('subscribed')

        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        self.mp_hands = mp.solutions.hands

        self.first_frame = True
        self.waiting = False
        self.wait_frame = 0
        self.objects = []
        self.distances = [200,200,200,200]
        self.found_object = False

    def show_video(self,window_name,ros_image):
        cv2.imshow(window_name,ros_image)
        cv2.waitKey(3)

    def convert_depth_image(self,ros_image):
        bridge = CvBridge()
        # Use cv_bridge() to convert the ROS image to OpenCV format
        try:
            self.depth_image = bridge.imgmsg_to_cv2(ros_image)
            # depth_array = np.array(self.depth_image, dtype=np.float32)

        except CvBridgeError:
            print(CvBridgeError())
            rospy.logerr("bad")

    def convert_color_image(self,ros_image):
        bridge = CvBridge()
        try:
            self.bgr_image = bridge.imgmsg_to_cv2(ros_image)
            self.hsv_image = cv2.cvtColor(self.bgr_image, cv2.COLOR_BGR2HSV)
            self.rgb_image = cv2.cvtColor(self.bgr_image, cv2.COLOR_BGR2RGB)
            self.grayscale_image = cv2.cvtColor(self.bgr_image, cv2.COLOR_BGR2GRAY)

            if self.waiting:
                self.wait_frame += 1
                print("waiting")
                if self.wait_frame == 60:
                    self.waiting = False
                    self.wait_frame = 0
            else:
                if self.first_frame == True:
                    self.objects = self.find_objects()
                    self.first_frame = False

                self.find_finger_vector()

                self.find_nearest_object()

                if self.found_object:
                    self.move_robot()
                    self.found_object = False
                    self.first_frame = True
                    self.waiting = True

            self.show_video("testtest", self.rgb_image)


        except CvBridgeError:
            print(CvBridgeError())
            rospy.logerr("bad")

    def find_objects(self):
        '''
        Prompts the user to select a background area.
        
        OR
        Canny edge detection into contour recognition - not sure how well it works
        # '''

        gauss_color = cv2.GaussianBlur(self.rgb_image, (3,3), cv2.BORDER_DEFAULT)

        # chans = cv2.split(gauss_color)
        canny_img = np.zeros(self.grayscale_image.shape)        

        canny_img = cv2.Canny(self.grayscale_image,50,150, 5, L2gradient = True)
        kernel = np.ones((15,15), np.uint8)

        canny_img = cv2.dilate(canny_img, kernel, iterations=1)
        # canny_img = cv2.erode(canny_img, kernel, iterations=1)
        
        # self.show_video("compared", canny_img)

        contours, hierarchy = cv2.findContours(canny_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        hull_list = []

        for i in range(len(contours)):
            hull = cv2.convexHull(contours[i])
            hull_list.append(hull)

        contours_by_area = sorted(contours, key=cv2.contourArea, reverse=True)
        self.hulls_by_area = sorted(hull_list, key=cv2.contourArea, reverse=True)

        object_locations = []
        self.object_coords = []

        for cnt in range(4):
            M = cv2.moments(self.hulls_by_area[cnt])
            cx = int(M['m10']/M['m00'])
            cy = int(M['m01']/M['m00'])

            self.object_coords.append((cx,cy))

            self.rgb_image = cv2.circle(self.rgb_image, (cx,cy), 3, (255,0,0), 3)

            location = self.get_xyz_from_image(self.depth_image, cx, cy)

            object_locations.append(location)

        cv2.drawContours(self.rgb_image, self.hulls_by_area, 0, (255,0,0), 3)
        cv2.drawContours(self.rgb_image, self.hulls_by_area, 1, (0,255,0), 3)
        cv2.drawContours(self.rgb_image, self.hulls_by_area, 2, (0,0,255), 3)
        cv2.drawContours(self.rgb_image, self.hulls_by_area, 3, (255,255,0), 3)

        rospy.logdebug(object_locations)

        # self.show_video("grayscale", self.grayscale_image)
        self.show_video("contours", self.rgb_image)

        return object_locations

    def find_finger_vector(self):
        with self.mp_hands.Hands(model_complexity=1, min_detection_confidence=0.35, min_tracking_confidence=0.35, max_num_hands=1) as hands:
            self.rgb_image.flags.writeable = False
            self.results = hands.process(self.rgb_image)
            self.rgb_image.flags.writeable = False

            h, w  = self.grayscale_image.shape

            if self.results.multi_hand_landmarks:
                for hand_landmarks in self.results.multi_hand_landmarks:
                    self.mp_drawing.draw_landmarks(self.rgb_image, hand_landmarks, self.mp_hands.HAND_CONNECTIONS, self.mp_drawing_styles.get_default_hand_landmarks_style(), self.mp_drawing_styles.get_default_hand_connections_style())
                pointer_fingertip = hand_landmarks.landmark[8]
                pointer_fingerbase = hand_landmarks.landmark[5]

                landmark_list = self.find_finger_coords(self.rgb_image)
                print(landmark_list[7][1:3])

                self.rgb_image = cv2.circle(self.rgb_image, landmark_list[8][1:3], 3, (0,0,255), 3)
                self.rgb_image = cv2.circle(self.rgb_image, landmark_list[5][1:3], 3, (255,0,0), 3)

                tip_coords = landmark_list[8][1:3]
                base_coords = landmark_list[5][1:3]

                self.fingertip_extended_coord = [base_coords[0] + 10*(tip_coords[0] - base_coords[0]), base_coords[1] + 10*(tip_coords[1] - base_coords[1])]
                self.rgb_image = cv2.line(self.rgb_image, base_coords, self.fingertip_extended_coord, (255,255,0), 3)
                self.rgb_image = cv2.line(self.rgb_image, base_coords, tip_coords, (0,255,0), 2)


                if tip_coords is not None and base_coords is not None:
                    self.tip = np.array(self.get_xyz_from_image(self.depth_image, tip_coords[0], tip_coords[1]))
                    self.base = np.array(self.get_xyz_from_image(self.depth_image, base_coords[0], base_coords[1]))


                    print("coordinates", tip_coords, " and", base_coords)

                    if np.linalg.norm(self.tip) > 10 and np.linalg.norm(self.base) > 10:
                        print('valid coords')
                        self.finger_vector = self.tip-self.base

                        self.fingertip_extended = self.tip + 10*(self.finger_vector)

                        print("vector from",self.base, " to", self.tip)

        for i in range(4):
            self.rgb_image = cv2.circle(self.rgb_image, self.object_coords[i], 3, (0,0,255), 3)

    def find_finger_coords(self, img):

        lmlist = []
        if self.results.multi_hand_landmarks:
            myHand = self.results.multi_hand_landmarks[0]
            for id, lm in enumerate(myHand.landmark):
                h, w, c = self.rgb_image.shape
                cx, cy = int(lm.x * w), int(lm.y * h)
                lmlist.append([id, cx, cy])
        return lmlist

    def find_nearest_object(self):

        for obj in range(len(self.objects)):
            try:
                d = np.linalg.norm(np.cross(self.fingertip_extended - self.base , np.subtract(self.base, self.objects[obj]) ) / np.linalg.norm(self.fingertip_extended - self.base))
                print("distance", d)
                self.distances[obj] = d
            except:
                pass
        
        if min(self.distances) < 30:
            r = np.argmin(self.distances)
            self.rgb_image = cv2.drawContours(self.rgb_image, self.hulls_by_area, r, (0,255,0), 3)
            print(self.objects[r])
            self.robot_coords = self.transform(self.objects[r])
            print('robot coords:',self.robot_coords)
            self.found_object = True

    def transform(self, camera_coords):
        result = np.array(camera_coords) + np.array([32, 269, 0])
        result  = np.multiply(result, [-1, 1, 1])
        return result

    def pointify(self, xyz):
        result = geometry_msgs.msg.Point()
        result.x = xyz[0] / 1000
        result.y = xyz[1] / 1000
        result.z = xyz[2]

        return result
        
    
    def move_robot(self):
        rospy.logdebug("MOVING TO HOME POSITION!!!")
        self.reset_service(0,0,0,0,0,0,0,False)

        rospy.logdebug("MOVING TO THE OBJECT!!!")
        point1 = self.pointify([self.robot_coords[0], self.robot_coords[1], 0.19309])
        self.step_service(point1, False)

        rospy.logdebug("PICKING UP THE OBJECT!!!")
        point2 = self.pointify([self.robot_coords[0], self.robot_coords[1], 0.023882])
        self.step_service(point2, True)

        rospy.logdebug("DROPPING OFF THE OBJECT!!!")
        point3 = self.pointify([0.24858,0,0.19309])
        self.step_service(point3,False)

        rospy.logdebug("MAYBE IT'S DONE???")




    def get_xyz_from_image(self,depth_image, x,y):
        intr = rs.intrinsics()
        intr.width = 1280
        intr.height = 720
        intr.ppx = 651.0391845703125
        intr.ppy = 354.0467834472656
        intr.fx = 921.5665283203125
        intr.fy = 921.62841796875
        intr.model = rs.distortion.none #"plumb_bob"
        intr.coeffs = [0.0, 0.0, 0.0, 0.0, 0.0]

        result = rs.rs2_deproject_pixel_to_point(intr, [x,y], depth_image[y,x])
        return result

if __name__ == '__main__':
    ''' The main() function. '''
    Vision()
    rospy.spin()